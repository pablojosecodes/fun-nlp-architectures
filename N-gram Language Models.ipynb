{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "At their core, n-gram language models can be represented by the following equation: $p(w_3|w_1, w_2) = \\frac{\\textit{count}(w_1,w_2, w_3)}{\\textit{count}(w_1, w_2)}$ .\n",
        "\n",
        "That is to say, they sample from a distribution based on conditional probabilities, and hold the Markov assumption.\n",
        "\n",
        "Let's build our own n-gram language model!"
      ],
      "metadata": {
        "id": "3jfGwJlrc-h8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "First we'll need to get our data ready. We'll use a shakespeare dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "MfrrCpxXdS-U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9wLcsoa-czib"
      },
      "outputs": [],
      "source": [
        "file = open(\"shake.txt\", \"r\")\n",
        "content = file.read()\n",
        "tokens = content.lower().split()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility functions"
      ],
      "metadata": {
        "id": "ExOhGQcR29RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ngrams(tokens, n):\n",
        "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
        "    return [\" \".join(ngram) for ngram in ngrams]\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# Create the models\n",
        "unigrams = generate_ngrams(tokens, 1)\n",
        "bigrams = generate_ngrams(tokens, 2)\n",
        "\n",
        "# Count occurrences\n",
        "unigram_model = defaultdict(int)\n",
        "bigram_model = defaultdict(int)\n",
        "\n",
        "for unigram in unigrams:\n",
        "    unigram_model[unigram] += 1\n",
        "\n",
        "for bigram in bigrams:\n",
        "    bigram_model[bigram] += 1\n",
        "\n",
        "bigram_probabilities = defaultdict(dict)\n",
        "\n",
        "for bigram in bigram_model:\n",
        "    w1, w2 = bigram.split()\n",
        "    bigram_probabilities[w1][w2] = bigram_model[bigram] / unigram_model[w1]\n",
        "\n",
        "\n",
        "\n",
        "def predict_next_word(word):\n",
        "    next_words = bigram_probabilities.get(word, {})\n",
        "    return max(next_words, key=next_words.get) if next_words else None\n",
        "\n",
        "def next_n_words(word, n):\n",
        "    sentence = word\n",
        "    for i in range(n):\n",
        "        word = predict_next_word(word)\n",
        "        sentence = sentence + \" \" + word\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "THhd4BIB280K"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out"
      ],
      "metadata": {
        "id": "5oCJNxp23H1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next_n_words(\"indeed\", 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OejRdlas08Ph",
        "outputId": "4ede7170-d9b9-41d7-ed19-a4f4a4e35df7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'indeed the king henry. i am'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_n_words(\"why\", 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-CdK7ReV1Amv",
        "outputId": "a20f4e6d-6576-430d-f725-0b89a76657ec"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'why should be a man of'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_n_words(\"how\", 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NvmJMEDq1CFF",
        "outputId": "1a477dbd-7246-4b9f-f91b-4decf34d47e5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'how now, my lord, i am'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm noticing a trend"
      ],
      "metadata": {
        "id": "-dWa0GKV3gNF"
      }
    }
  ]
}